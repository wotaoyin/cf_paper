% !TEX root = ./amsa_main.tex
\subsection{Operator Splitting Schemes}\label{sec:splitting}
Before we apply our discussions above to operator splitting schemes for new algorithms, we review several major operator splitting schemes and discuss their CF properties by using the results from \S\ref{sc:comb}. For basic operator concepts like monotonicity and cocoercivity, please refer to Appendix \ref{sec:op-concept}. The definitions of resolvent and reflective-resolvent operators $\cJ_{\cA}$ and $\cR_{\cA}$ are also given there, in \eqref{def-resolvent} and \eqref{def-ref} respectively. \cut{The coordinate update methods we propose, based on operator splitting schemes, all have convergence guarantee, at least for async-parallel (thus also stochastic) update \cite{Peng_2015_AROCK}. However, naively extending existing convergent algorithms to coordinate update schemes may result in divergence or wrong solutions; see also Appendix  \ref{sec:op-concept} for a counterexample.}  %goes over the definitions and properties of operators that are used to build  and ensure their convergence.

Consider the following problem: given three operators $\cA,\cB,\cC$, possibly set-valued,  \begin{equation}\label{eqn:3s_problem}
\text{find } x \in \HH \qquad \text{ such that }  \qquad 0 \in \cA x + \cB x +\cC x,
\end{equation}
This is a high-level abstraction of many problems. The study began in the 1960s, and since then a large number of algorithms and applications have been introduced. We review a few general methods in this subsection.

When $\cA, \cB$ are maximally monotone (think it as the subdifferential $\partial f$ of a proper convex function $f$) and $\cC$ is $\beta$-cocoercive (think it as the gradient $\nabla f$ of a $1/\beta$-Lipschitz differentiable function $f$),  a solution can be found by the iteration \eqref{fpi} with $\cT=\TS$, introduced recently in \cite{davis2015three}, where  \cut{three operator splitting (3S) for solving \eqref{eqn:3s_problem} is defined by}
\beq\label{3s}
\TS := \cI- \cJ_{\gamma \cB}+ \cJ_{\gamma \cA}\circ(2 \cJ_{\gamma \cB}- \cI - \gamma \cC\circ \cJ_{\gamma \cB}).
\eeq \cut{It can be shown that if $\cC$ is $\beta$-cocoercive, then}Indeed, by setting  $\gamma\in(0,2\beta)$, $\cT_{3S}$ is $(\frac{2\beta}{4\beta-\gamma})$-averaged (think it as a weaker property than the Picard contraction that does not guarantee $\cT$ having a fixed point). Following the standard convergence result (cf. textbook \cite{B-C2011cvx-mon}), provided that $\cT$ has a  fixed point, the sequence from~\eqref{fpi} converges to a fixed-point $x^*$ of $\TS$. Instead of $x^*$, $\cJ_{\gamma\cB}(x^*)$ is a solution to~\eqref{eqn:3s_problem}.  \cut{, and one can choose $\gamma\in(0,2\beta)$ for convergence.}
Applying the results in \S\ref{sc:comb},  $\TS$ {is CF if } $\cJ_{\gamma \cA}$ is separable ($\cC_1$), $\cJ_{\gamma \cB}$ is \cut{easy-to-compute or }Type-II CF ($\cF_2$), and $\cC$ is Type-I CF ($\cF_1$). %Given $x$, $(\cT_{3S} x)_i = ......$

%\subsubsection{Special cases}
We give a few special cases of $\TS$ below, which have much longer history. They all converge to a fixed point $x^*$ whenever a solution exists and $\gamma$ is properly chosen. Whenever $\cB\neq 0$, {$\cJ_{\gamma \cB}(x^*)$ replaces $x^*$  as a solution to \eqref{eqn:3s_problem}.

 \textbf{Forward-Backward Splitting (FBS):} Letting $\cB=0$ yields $\cJ_{\gamma\cB}=\cI$. Then, $\TS$ reduces to FBS \cite{passty1979FBS}:
 \begin{equation}\label{eq:FBS}
 \TFBS:=\cJ_{\gamma \cA}\circ(\cI-\gamma \cC)
 \end{equation}
 for solving the problem $0\in\cA x+\cC x$.

\textbf{Backward-Forward Splitting (BFS):} Letting $\cA=0$ yields $\cJ_{\gamma\cA}=\cI$. Then, $\TS$ reduces to BFS:
  \begin{equation}\label{eq:BFS}\TBFS:=(\cI-\gamma \cC)\circ \cJ_{\gamma \cB}
  \end{equation}
for solving the problem $0\in\cB x+\cC x$. When $\cA=\cB$, $\TFBS$ and $\TBFS$ apply the same pair of operators in the opposite orders, and they solve the same problem. Iterations based on $\TBFS$ are rarely used in the literature because they  need an extra application of $\cJ_{\gamma B}$ before returning the solution, so $\TBFS$ is seemingly an unnecessary variant of $\TFBS$. However, they become  different in coordinate update algorithms; in particular, $\TBFS$ is CF (but $\TFBS$ is generally not) when $\cJ_{\gamma \cB}$ is \cut{easy-to-compute or }Type-II CF ($\cF_2$) and $\cC$ is Type-I CF ($\cF_1$). Therefore, $\TBFS$ is worth discussing alone.

\textbf{Douglas-Rachford Splitting (DRS):} Letting $\cC=0$, $\TS$ reduces to 
  \begin{equation}\label{eq:DRS}\TDRS:=\cI-\cJ_{\gamma \cB}+\cJ_{\gamma\cA}\circ(2\cJ_{\gamma \cB}-\cI)=\frac{1}{2}(\cI+\cR_{\gamma\cA}\circ \cR_{\gamma\cB})
  \end{equation}
introduced in \cite{douglas1956DRS} for solving the problem $0\in\cA x+\cB x$. A more general splitting is the  relaxed Peaceman-Rachford splitting (RPRS) with $\lambda\in[0,1]$:
 \begin{equation}
\cT_{\text{RPRS}} = (1 - \lambda)\, \cI + \lambda \, \cR_{\gamma \cA} \circ \cR_{\gamma \cB},
\end{equation}
which recovers $\TDRS$ by setting $\lambda=\frac{1}{2}$ and Peaceman-Rachford splitting (PRS) \cite{peaceman1955PRS} by $\lambda=1$.

\textbf{Forward-Douglas-Rachford Splitting (FDRS):} Let $V$ be  a linear subspace, and $\cN_V$ and $\cP_V$ be the normal cone and projection operators, respectively. Letting $\cB=\cN_V$ and $\cC=\cP_V\circ\tilde{\cC}\circ\cP_V$, $\TS$ reduces to
 $$\TFDRS=\cI-\cP_V+\cJ_{\gamma\cA}\circ(2\cP_V-\cI-\gamma \cP_V\circ\tilde{\cC}\circ\cP_V)$$
introduced in \cite{briceno2015FDRS} for solving the problem $0\in\cA\,x+\tilde{\cC}\,x+\cN_V\,x$.

\textbf{Forward-Backward-Forward Splitting (FBFS):} Composing $\TFBS$ with one more forward step gives $\TFBFS$ introduced in \cite{FBF_Tseng}:
\begin{align}\label{eqn:fbf}
\TFBFS = -\gamma \cC  + (\cI-\gamma \cC)\cJ_{\gamma \cA}(\cI-\gamma \cC).
\end{align} %~\cite{FBF_Tseng} for solving the problem of finding the zero in the sum of two operators, i.e.,
%\begin{equation}\label{eqn:2_problem}
%\text{find } x \in \HH \qquad \text{ such that }  \qquad 0 \in \cA\,x + \cC\,x,
%\end{equation}
%is defined in the following 
$\TFBFS$ is not  a special case of $\TS$. At the expense of one more application of $(\cI-\gamma \cC)$, $\TFBFS$ relaxes the convergence condition  of $\TFBS$ from  the cocoercivity of $\cC$ {to its monotonicity (for example, a nonzero skew symmetric matrix is monotonic but not cocoercive)}. 
From Table \ref{table:comp-op}, we know that $\TFBFS$ is CF if both $\cC$ and $\cJ_{\gamma \cA}$ are separable.

\subsubsection{Examples in Optimization}
Consider the optimization problem
\begin{equation}\label{eq:opt-example}
\Min_{x\in X}\, f(x)+g(x),
\end{equation}
where $X$ is the feasible set and $f$ and $g$ are objective functions. We present examples of operator splitting methods discussed above.

\cut{We discuss a few well-known optimization methods for solving problems in the form of \eqref{eq:opt-example} and relate them to the previous operator splitting methods.}

\begin{example}[{proximal gradient method}]\label{alg:prox-grad} Let $X=\RR^m$, $f$ be differentiable, and $g$ be proximable in \eqref{eq:opt-example}. Letting $\cA=\partial g$ and $\cC=\nabla f$ in \eqref{eq:FBS} gives $\cJ_{\gamma \cA}=\prox_{\gamma g}$ and reduces $x^{k+1}=\TFBS(x^k)$ to prox-gradient iteration: \cut{. one can apply the proximal gradient method with update}
\begin{equation}\label{eq:prox-grad}x^{k+1}=\prox_{\gamma g}(x^k-\gamma\nabla f(x^k)).
\end{equation} 
A special case of \eqref{eq:prox-grad} with $g=\iota_X$ is the projected gradient iteration:
\begin{equation}\label{eq:proj-grad}
x^{k+1}=\cP_X(x^k-\gamma \nabla f(x^k)).
\end{equation}

If $\nabla f$ is CF and $\prox_{\gamma g}$ is (nearly-)separable (e.g., $g(x)=\|x\|_1$ or the indicator function of a box constraint) or if $\nabla f$ is Type-II CF and $\prox_{\gamma g}$ is cheap (e.g., $\nabla f(x)=Ax-b$ and $g=\|x\|_2$), the FBS iteration in \eqref{eq:prox-grad} is CF. In the latter case, we can also apply the BFS iteration \eqref{eq:BFS} (i.e,  compute $\prox_ {\gamma g}$ and then the gradient update), which is also CF.

\end{example}

%\begin{example}[\textbf{Projected gradient method}]\label{alg:proj-grad} Let $f$ be differentiable and $g=0$. Then, \eqref{eq:opt-example} is a constrained smooth convex program. Let  $\cA=\cN_X$ and $\cC=\nabla f$ in \eqref{eq:FBS}. Then,\cut{ For this problem, one can apply} $x^{k+1}=\TFBS(x^k)$ recovers the projected gradient iteration:
%$$x^{k+1}=\cP_X(x^k-\gamma \nabla f(x^k)).$$
%%{Then $\cJ_{\gamma \cA}=\cP_X$, and the above update is simply $x^{k+1}=\textcolor{blue}{\TFBS}(x^k)$.}
%If $\nabla f$ is CF (e.g., $\nabla f(x)=Ax-b$) and $X$ is (nearly-)separable constraint (e.g., box constraint), the iteration is CF.
%\end{example}

\DIFdelbegin %DIFDELCMD < \begin{example}[ADMM]%%%
\DIFdelend \DIFaddbegin \begin{example}[alternating direction method of multipliers (ADMM)]\DIFaddend \label{alg:admm} Setting $X=\RR^m$ simplifies \eqref{eq:opt-example} to 
\begin{equation}\label{eq:compx-y}
\Min_{x,y}~f(x)+g(y),\quad\St~ x-y=0.
\end{equation} 
%and one can apply the alternating direction method of multiplier (ADMM) by the updates
The ADMM method iterates:
\begin{subequations}\label{eq:admmx-y}
\begin{align}
&x^{k+1}=\prox_{\gamma f}(y^k-\gamma s^k),\\%\argmin_x f(x)+\langle s^k, x-y^k\rangle + \frac{1}{2\eta}\|x-y^k\|^2,\\
&y^{k+1}=\prox_{\gamma g}(x^{k+1}+\gamma s^{k}),\\%\argmin_y g(y)+\langle s^k, x^{k+1}-y\rangle + \frac{1}{2\eta}\|x^{k+1}-y\|^2,\\
&s^{k+1}=s^k+\frac{1}{\gamma}(x^{k+1}-y^{k+1}).
\end{align}
\end{subequations}
(The iterations can be generalized to handle the constraints $Ax-By=b$.) The dual problem of \eqref{eq:compx-y} is $\min_s f^*(-s)+g^*(s)$, where $f^*$ is the convex conjugate of $f$. Letting $\cA=-\partial f^*(-\cdot)$ and $\cB=\partial g^*$ in \eqref{eq:DRS} recovers the iteration  \eqref{eq:admmx-y} through (see the derivation in Appendix \ref{sec:drs-admm})
\begin{align*}
%&s^k=\cJ_{\gamma \cB}(t^k),\\
&t^{k+1}=\TDRS(t^k)=t^k-\cJ_{\gamma \cB}(t^k)+\cJ_{\gamma\cA}\circ(2\cJ_{\gamma \cB}-\cI)(t^k).
\end{align*}  %If $\prox_{\gamma f}$ is easy-to-compute, and $\prox_{\gamma g}$ is separable, then the ADMM iteration  is CF.
From the results in \S\ref{sc:comb}, a sufficient condition for the above iteration to be CF is that $\cJ_{\gamma\cA}$ is (nearly-)separable and $\cT_{\gamma\cB}$ being CF. 
\end{example}


%\begin{subsection}{Forward-backward splitting (FBS) and backward-forward splitting (BFS)}
%The forward-backward splitting (FBS) and the backward-forward splitting (BFS) solves the problem of finding the zero in the sum of two maximally monotone operators, i.e., it considers
%\begin{equation}\label{eqn:monotone_problem}
%\text{find } x \in \cH \qquad \text{ such that }  \qquad 0 \in \cA\,x + \cB\,x.
%\end{equation}
%The forward-backward splitting operator is the following
%\begin{equation}\label{eqn:fbs}
%\cT_{\text{FBS}} \, x= \cJ_{\gamma \cA} \circ (x - \gamma \cB \, x),
%\end{equation}
%and the backward-forward splitting operator is
%\begin{equation}\label{eqn:fbs}
%\cT_{BFS} \, x= (I - \gamma \cB) \circ \cJ_{\gamma \cA}(x),
%\end{equation}
%where $\cJ_{\gamma \cA} := (1+\gamma \, \cA)^{-1}$ is the \emph{resolvant} of $A$. It has been shown that $x$ solves \eqref{eqn:monotone_problem} if any only if $x$ is a fixed point of $\cT_{\text{FBS}}$ or $\cT_{BFS}$. As we discussed before, if $\cB$ is BC-friendly, and $J_{\gamma \cA}$ is block-separable, then $\cT_{\text{FBS}}$ is BC-friendly. However, if $J_{\gamma \cA}$ is easy-to-compute/update but not necessary block-separable, and $B$ is BC-friendly, then the $\cT_{\text{FBS}}$ is not necessary BC-friendly, but $\cT_{\text{BFS}}$ is BC-friendly. We will give some examples in Section \ref{sec:app} to illustrate the two different types of update scheme. 
%
%\end{subsection}

\cut{
\begin{subsection}{Relaxed Peaceman-Rachford splitting (RPRS)}
The relaxed Peaceman-Rachford splitting (RPRS) also targeted at solving \eqref{eqn:monotone_problem}. The RPRS is define in the following
\begin{equation}
\cT_{\text{RPRS}} = (1 - \lambda)\, \cI + \lambda \, \cR_{\gamma \cA} \circ \cR_{\gamma \cB},
\end{equation}
where $\cR_{\gamma \cA}:= 2 J_{\gamma \cA} - I$ is the reflection operator. Note that $\lambda = 1$ gives the Peaceman-Rachford splitting (PRS) operator, and $\lambda = \frac{1}{2}$ gives the Douglas-Rachford splitting (DRS) operator. 

Applying the results of composite maps, the RPRS splitting operator is CF in either of the following two cases:

Case 1. $ \cR_{\gamma A}$ is block-separable  and $\cR_{\gamma B}$ is CF.

Case 2. $ \cR_{\gamma A}$ is Type-I CF and $\cR_{\gamma B}$ is easy-to-update. 
\end{subsection}

\subsection{Forward-Backward-Forward operator splitting}
The Forward-Backward-Forward operator splitting (FBF)~\cite{FBF_Tseng} for solving the problem of finding the zero in the sum of two operators, i.e.,
\begin{equation}\label{eqn:2_problem}
\text{find } x \in \HH \qquad \text{ such that }  \qquad 0 \in \cA\,x + \cC\,x,
\end{equation}
is defined in the following 
\begin{align}\label{eqn:fbf}
\cT_{\text{FBF}} = -\gamma \cC  + (I-\gamma \cC)\cJ_{\gamma \cA}(I-\gamma \cC).
\end{align}
The advantage of FBF comparing to FBS is that the cocoercivity condition of $\cC$ is relaxed into monotonicity at the expense of additional computations.
Then, we know that $\cT_{\text{FBF}}$ is CF, if both $\cC$ and $\cJ_{\gamma \cA}$ are separable.
}

%\begin{subsection}{Three operator splitting (3S)}
%The three operator splitting method solves the problem of finding the zero in the sum of three operators ($A, B, C$), where $A, B$ are maximally monotone and $C$ is cocoercive. The three operator splitting is defined as following:
%$$\cT_{3S} = \cI- \cJ_{\gamma \cB}- \cJ_{\gamma \cA}\circ(2 \cJ_{\gamma \cB}- \cI - \gamma \cC\circ \cJ_{\gamma \cB}).$$
%Then, we know that $\cT$ is BC-friendly, if $\cJ_{\gamma \cA}$ is separable, $\cJ_{\gamma \cB}$ is easy-to-compute/update and $\cC$ is BC-friendly. 
%
%\end{subsection}



%\end{section} reduces to FBS \cite{passty1979FBS}
